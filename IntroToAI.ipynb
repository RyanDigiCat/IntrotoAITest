{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "init_cell": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code for this notebook.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Interactivity imports for buttons in the notebook\n",
    "from IPython.display import HTML, Javascript, display\n",
    "import IPython\n",
    "from ipywidgets import interact, widgets\n",
    "import random\n",
    "\n",
    "# Create button that runs the below cell\n",
    "def run_below(ev):\n",
    "    display(Javascript('IPython.notebook.execute_cells([IPython.notebook.get_selected_index()+1])'))\n",
    "\n",
    "run_code_below_button = widgets.Button(description=\"Run code\")\n",
    "run_code_below_button.on_click(run_below)\n",
    "\n",
    "# Create toggle code button\n",
    "def toggle_code(for_next=False):\n",
    "    this_cell = \"\"\"$('div.cell.code_cell.rendered.selected')\"\"\"\n",
    "    next_cell = this_cell + '.next()'\n",
    "\n",
    "    toggle_text = 'Toggle show/hide code'  # text shown on toggle link\n",
    "    target_cell = this_cell  # target cell to control with toggle\n",
    "    js_hide_current = ''  # bit of JS to permanently hide code in current cell (only when toggling next cell)\n",
    "\n",
    "    if for_next:\n",
    "        target_cell = next_cell\n",
    "        # toggle_text += ' next cell'\n",
    "        js_hide_current = this_cell + '.find(\"div.input\").hide();'\n",
    "\n",
    "    js_f_name = 'code_toggle_{}'.format(str(random.randint(1,2**64)))\n",
    "\n",
    "    html = \"\"\"\n",
    "        <script>\n",
    "            function {f_name}() {{\n",
    "                {cell_selector}.find('div.input').toggle();\n",
    "            }}\n",
    "\n",
    "            {js_hide_current}\n",
    "        </script>\n",
    "\n",
    "        <a href=\"javascript:{f_name}()\">{toggle_text}</a>\n",
    "    \"\"\".format(\n",
    "        f_name=js_f_name,\n",
    "        cell_selector=target_cell,\n",
    "        js_hide_current=js_hide_current, \n",
    "        toggle_text=toggle_text\n",
    "    )\n",
    "\n",
    "    return HTML(html)\n",
    "\n",
    "# Create the run code and toggle code buttons for the cell below\n",
    "def create_buttons():\n",
    "    display(run_code_below_button)\n",
    "    display(toggle_code(for_next=True))\n",
    "    \n",
    "# Create a button that hides all the code in the notebook and autoruns on the notebook\n",
    "hide_all_code_button = HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code for this notebook.\"></form>''')\n",
    "display(hide_all_code_button)\n",
    "\n",
    "\n",
    "# CODE BELOW WAS SUPPOSED TO BE A SMARTER TOGGLE BUTTON - DOESNT WORK YET\n",
    "# javascript_functions = {False: \"hide()\", True: \"show()\"}\n",
    "# button_descriptions  = {False: \"Show code\", True: \"Hide code\"}\n",
    "# STATE = False\n",
    "# def toggle_code(ev):\n",
    "#     import random\n",
    "#     for_next = True\n",
    "#     this_cell = \"\"\"$('div.cell.code_cell.rendered.selected')\"\"\"\n",
    "#     next_cell = this_cell + '.next()'\n",
    "\n",
    "#     toggle_text = 'Toggle show/hide'  # text shown on toggle link\n",
    "#     target_cell = this_cell  # target cell to control with toggle\n",
    "#     js_hide_current = ''  # bit of JS to permanently hide code in current cell (only when toggling next cell)\n",
    "\n",
    "#     if for_next:\n",
    "#         target_cell = next_cell\n",
    "#         toggle_text += ' next cell'\n",
    "#         js_hide_current = this_cell + '.find(\"div.input\").hide();'\n",
    "\n",
    "#     js_f_name = 'code_toggle_{}'.format(str(random.randint(1,2**64)))\n",
    "\n",
    "#     output = \"\"\"\n",
    "#         <script>\n",
    "#             function {f_name}() {{\n",
    "#                 {cell_selector}.find('div.input').toggle();\n",
    "#             }}\n",
    "\n",
    "#             {js_hide_current}\n",
    "#         </script>\n",
    "\n",
    "#         <a href=\"javascript:{f_name}()\"></a>\n",
    "#     \"\"\".format(\n",
    "#         f_name=js_f_name,\n",
    "#         cell_selector=target_cell,\n",
    "#         js_hide_current=js_hide_current, \n",
    "#     )\n",
    "\n",
    "#     display(HTML(output))\n",
    "\n",
    "\n",
    "# def button_action(value):\n",
    "\n",
    "#     \"\"\"\n",
    "#     Calls the toggle_code function and updates the button description.\n",
    "#     \"\"\"\n",
    "\n",
    "#     state = value.new\n",
    "\n",
    "#     toggle_code()\n",
    "#     # Change the state\n",
    "#     state = not state\n",
    "\n",
    "#     value.owner.description = button_descriptions[state]\n",
    "    \n",
    "# def create_buttons():\n",
    "#     state = False\n",
    "#     #toggle_code(state)\n",
    "#     code_toggle_button = widgets.ToggleButton(description = button_descriptions[state])\n",
    "#     code_toggle_button.observe(button_action, \"value\")\n",
    "#     display(widgets.HBox([run_code_below_button, code_toggle_button]))\n",
    "\n",
    "\n",
    "\n",
    "# ALSO DOESNT WORK YET\n",
    "# # Create toggle code button\n",
    "# def toggle_code(for_next=True):\n",
    "#     this_cell = \"\"\"$('div.cell.code_cell.rendered.selected')\"\"\"\n",
    "#     next_cell = this_cell + '.next()'\n",
    "\n",
    "#     toggle_text = 'Toggle show/hide code'  # text shown on toggle link\n",
    "#     target_cell = this_cell  # target cell to control with toggle\n",
    "#     js_hide_current = ''  # bit of JS to permanently hide code in current cell (only when toggling next cell)\n",
    "\n",
    "#     if for_next:\n",
    "#         target_cell = next_cell\n",
    "#         # toggle_text += ' next cell'\n",
    "#         js_hide_current = this_cell + '.find(\"div.input\").hide();'\n",
    "\n",
    "#     js_f_name = 'code_toggle_{}'.format(str(random.randint(1,2**64)))\n",
    "\n",
    "#     html = \"\"\"\n",
    "#         <script>\n",
    "#             function {f_name}() {{\n",
    "#                 {cell_selector}.find('div.input').toggle();\n",
    "#             }}\n",
    "\n",
    "#             {js_hide_current}\n",
    "#         </script>\n",
    "\n",
    "#         <a href=\"javascript:{f_name}()\">{toggle_text}</a>\n",
    "#     \"\"\".format(\n",
    "#         f_name=js_f_name,\n",
    "#         cell_selector=target_cell,\n",
    "#         js_hide_current=js_hide_current, \n",
    "#         toggle_text=toggle_text\n",
    "#     )\n",
    "\n",
    "#     return HTML(html)\n",
    "\n",
    "# toggle_code_below_button = widgets.Button(description=\"Show/hide code\")\n",
    "# toggle_code_below_button.on_click(toggle_code)\n",
    "\n",
    "# def create_buttons():\n",
    "#     display(widgets.HBox([run_code_below_button, toggle_code_below_button]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Basic dependencies\n",
    "import time\n",
    "\n",
    "# Numerical and dataframe dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "import altair as alt\n",
    "#alt.renderers.enable('notebook')\n",
    "from livelossplot import PlotLosses\n",
    "from livelossplot.tf_keras import PlotLossesCallback\n",
    "\n",
    "# Machine Learning libraries\n",
    "import sklearn.metrics as metrics\n",
    "import tensorflow as tf\n",
    "from tf_explain.core.grad_cam import GradCAM\n",
    "\n",
    "# Library settings\n",
    "tf.random.set_seed(42)\n",
    "tf.get_logger().setLevel('INFO') # Ignore warning/depreciation messages in the logger\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Set overall global variables\n",
    "HEIGHT = 32\n",
    "WIDTH = 32\n",
    "NUM_CHANNELS = 3\n",
    "IMG_SHAPE = (HEIGHT, WIDTH, NUM_CHANNELS)\n",
    "CLASS_NAMES = {\n",
    "    0: 'airplane',\n",
    "    1: 'automobile',\n",
    "    2: 'bird',\n",
    "    3: 'cat',\n",
    "    4: 'deer',\n",
    "    5: 'dog',\n",
    "    6: 'frog',\n",
    "    7: 'horse',\n",
    "    8: 'ship',\n",
    "    9: 'truck'\n",
    "}\n",
    "NUM_CLASSES = len(CLASS_NAMES) # 10\n",
    "\n",
    "SAMPLE_SIZE = 101 # The number of samples to plot for the image sliders\n",
    "\n",
    "DATA_LOADED = False # Add in more of these?!?!\n",
    "DATA_PREPROCESSED = False\n",
    "BASELINE_MODEL_BUILT = False\n",
    "NN_CONFUSION_MATRIX = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def get_model_outputs(model, inputs):\n",
    "    model_probs = model.predict(inputs)\n",
    "    model_preds = model_probs.argmax(axis=1)\n",
    "    return model_probs, model_preds\n",
    "\n",
    "def print_confusion_matrix(predicted_labels, true_labels, class_names, figsize = (20,10), fontsize=14):\n",
    "    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    confusion_matrix: numpy.ndarray\n",
    "        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n",
    "        Similarly constructed ndarrays can also be used.\n",
    "    class_names: list\n",
    "        An ordered list of class names, in the order they index the given confusion matrix.\n",
    "    figsize: tuple\n",
    "        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n",
    "        the second determining the vertical size. Defaults to (10,7).\n",
    "    fontsize: int\n",
    "        Font size for axes labels. Defaults to 14.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.figure.Figure\n",
    "        The resulting confusion matrix figure\n",
    "    \"\"\"\n",
    "    confusion_matrix = metrics.confusion_matrix(true_labels, predicted_labels)\n",
    "    confusion_matrix_norm = confusion_matrix.astype('float') / confusion_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    cm_sum = np.sum(df_cm, axis=1)\n",
    "    cm_perc = df_cm / cm_sum.astype(float) * 100\n",
    "    \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    annot = np.empty_like(df_cm).astype(str)\n",
    "    nrows, ncols = df_cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = df_cm.iloc[i][j]\n",
    "            p = cm_perc.iloc[i][j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
    "            elif c == 0:\n",
    "                annot[i, j] = ''\n",
    "            else:\n",
    "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
    "    \n",
    "    heatmap = sns.heatmap(df_cm, annot=annot, fmt='')\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "def checkpoint(checkpoint_number=None):\n",
    "    assert isinstance(checkpoint_number, int), 'The checkpoint number must be an integer!'\n",
    "    vars_to_check = [\n",
    "        DATA_LOADED,\n",
    "        DATA_PREPROCESSED,\n",
    "        BASELINE_MODEL_BUILT,\n",
    "        NN_CONFUSION_MATRIX\n",
    "    ]\n",
    "    error_messages = [\n",
    "        \"Please first load the data\",\n",
    "        \"Please preprocess the data before you train the neural networks\",\n",
    "        \"Please first build the baseline Neural Network model\",\n",
    "        \"Please first run the confusion matrix\"\n",
    "    ]\n",
    "    \n",
    "    error_message_endstring = \"\"\" \\n\n",
    "    (This is probably just because you missed a button, so try clicking the button above and then clicking this\n",
    "    one again. If that doesnt work, try pressing all of the above buttons in order and rerunning this cell again \n",
    "    by pressing the button)\n",
    "    \"\"\"\n",
    "    \n",
    "    for var_to_check, error_message in list(zip(vars_to_check, error_messages))[:checkpoint_number]:\n",
    "        assert var_to_check, \" \".join([error_message, error_message_endstring])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 🤖 Intro to AI Notebook 🤖"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Remark: Any references to other Digital Catapult talk offerings will consistently be labelled as **<span style=\"text-decoration:underline;\">Talk X</span>** throughout this notebook. Please don’t hesitate to ask a member of the Digital Catapult team for more information if you are interested._\n",
    "\n",
    "<img src=\"images/IntroToAIOpeningImage.jpeg\" width=\"600\">\n",
    "\n",
    "Welcome to the _JuPyter notebook_ ! This is the classic environment for a Data Scientist and Machine Learning Engineer. This is often where we experiment with pieces of code, look and analyse data and build machine learning models. It is also a useful tool for presenting the findings of our analyses in a clear manner.\n",
    "\n",
    "We have set this notebook up in a special and unusual way today - all the code cells will be hidden, so you don't have to worry about it. But, if you would like to take a look then you can toggle the code on and off at any time!\n",
    "\n",
    "You can also toggle on and off the code in the entire notebook with the button at the top.\n",
    "\n",
    "The way we will interact with this notebook is as below:\n",
    "\n",
    "\n",
    "\n",
    "*   The **\"run code\"** cell will run the code in the hidden cell;\n",
    "*   The **toggle code** button will allow you to see the underlying code, if you wish\n",
    "\n",
    "You can try out these buttons below! The buttons below will print out the phrase ```Hello world``` - any good programmers first piece of code!\n",
    "\n",
    "> **The button below will print out the phrase** ```Hello world```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "init_cell": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd34924441484d009330ab6754bc08e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run code', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_12841850720977302611() {\n",
       "                $('div.cell.code_cell.rendered.selected').next().find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            $('div.cell.code_cell.rendered.selected').find(\"div.input\").hide();\n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_12841850720977302611()\">Toggle show/hide code</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_buttons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hello world!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## §0: Introduction\n",
    "\n",
    "The intention of this notebook is to give you an interactive and friendly introduction to AI and Machine Learning by stepping through a classic data science/Machine Learning workflow.\n",
    "\n",
    "<img src=\"images/DataScienceLifeCycle.jpg\" width=\"800\" title=\"Data Science Workflow\">\n",
    "\n",
    "A classic Machine Learning workflow is made up of the following general steps:\n",
    "\n",
    "\n",
    "\n",
    "*   Discovery phase\n",
    "*   Data Preparation Phase\n",
    "*   Model Iteration / Development Phase\n",
    "*   Analysis and communication of results\n",
    "*   Operationalisation and Deployment\n",
    "\n",
    "These are discussed in more detail in the **_<span style=\"text-decoration:underline;\">Data Science Process</span>_** talk and workshop.\n",
    "\n",
    "\n",
    "## §1: Discovery Phase\n",
    "\n",
    "While this is an important element of any good Machine Learning project (see the \"Data Science Process\" talk), we defer the details for this workshop. In particular, suppose the outcomes of this were as follows:\n",
    "\n",
    "\n",
    "### §1.1 Problem Definition\n",
    "\n",
    "The problem definition that business X cares about is the classification of images. In particular, they want to develop a system that will take in RGB coloured images, and classify them into one of 10 pre-defined classes. These classes are:\n",
    "\n",
    "\n",
    "\n",
    "*   airplane\n",
    "*   automobile\n",
    "*   bird\n",
    "*   cat\n",
    "*   deer\n",
    "*   dog\n",
    "*   frog\n",
    "*   horse\n",
    "*   ship\n",
    "*   truck\n",
    "\n",
    "\n",
    "#### §1.1.1 Inputs\n",
    "\n",
    "The input will be a 32x32 pixel RGB colour photograph of objects from these 10 classes\n",
    "\n",
    "\n",
    "#### §1.1.2 Outputs\n",
    "\n",
    "The output of the Machine Learning model will be a prediction of the class\n",
    "\n",
    "\n",
    "### §1.2 Data Mapping\n",
    "\n",
    "For the purposes of this exercise, we will defer data mapping. We can assume that the data has already been collected. However, it is worth stressing that this is a really important part of the process. The other talks such as **_<span style=\"text-decoration:underline;\">Data for AI</span> _**and **_<span style=\"text-decoration:underline;\">Data Science Process</span>_** elaborate on this more, but it is important to think about whether or not the problem you have is suitable for machine learning before embarking on it to derisk it, and what datasets are available to you.\n",
    "\n",
    "\n",
    "## §2: Data Preparation Phase\n",
    "\n",
    "In this phase, we will prepare our inputs (defined above) ready for the models to be built. We will also perform some _\"Exploratory Data Analysis\"_ (EDA) to look at our data and the type of data that we have.\n",
    "\n",
    "\n",
    "### §2.1: Data Collection\n",
    "\n",
    "Usually, we have to track down our data which might be sitting in many different internal systems. To understand the types of data that are suitable for machine learning, you can learn more in the **_<span style=\"text-decoration:underline;\">Data for AI</span>_** talk.\n",
    "\n",
    "For our problem, we will work with image data, so our dataset is a collection of images. As for data collection, we will be using an _open source_ (public) dataset that anyone can access, so we don’t have any data collection issues. We will be working with the CIFAR dataset.\n",
    "\n",
    "CIFAR is an acronym that stands for the [Canadian Institute For Advanced Research](https://www.cs.toronto.edu/~kriz/cifar.html) and the CIFAR-10 dataset was developed along with the CIFAR-100 dataset by researchers at the CIFAR institute.\n",
    "\n",
    "The dataset is comprised of 60,000 32x32 pixel color photographs of objects from 10 classes, such as frogs, birds, cats, ships, etc. We associate the class labels with integer values as below:\n",
    "\n",
    "\n",
    "\n",
    "*   0: airplane\n",
    "*   1: automobile\n",
    "*   2: bird\n",
    "*   3: cat\n",
    "*   4: deer\n",
    "*   5: dog\n",
    "*   6: frog\n",
    "*   7: horse\n",
    "*   8: ship\n",
    "*   9: truck\n",
    "\n",
    "Now, we get the machine learning model to predict the correct “number”, which we can get the name of the corresponding class via this mapping.\n",
    "\n",
    "These are very small images, much smaller than a typical photograph, and the dataset was intended for computer vision research.\n",
    "\n",
    "CIFAR-10 is a well-understood dataset and widely used for benchmarking computer vision algorithms in the field of machine learning. The problem is _“solved”_. It is relatively straightforward to achieve 80% classification accuracy. Top performance on the problem is achieved by deep learning convolutional neural networks with a classification accuracy of around 97% on the test dataset. Human accuracy is around 94% [1].\n",
    "\n",
    "Today, we will show you how you can build an image recognition classifier to be able to achieve human accuracy.\n",
    "\n",
    "First, we have to download the dataset!\n",
    "\n",
    "> **The button below will download the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "init_cell": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd34924441484d009330ab6754bc08e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run code', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_10898405889396430660() {\n",
       "                $('div.cell.code_cell.rendered.selected').next().find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            $('div.cell.code_cell.rendered.selected').find(\"div.input\").hide();\n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_10898405889396430660()\">Toggle show/hide code</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_buttons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR-10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Really we should also create a validation set - check if we want to do this or will it be too confusing?\n",
    "# NUM_VAL_IMAGES = 10000\n",
    "# from sklearn.model_selection import train_test_split \n",
    "# We will use this function to split our \"training\" set into our \"training\" + \"validation\" set - confusing name!\n",
    "# (X_train, X_val), (y_train, y_val) = train_test_split(X_train, y_train, stratify=y_train, test_size=NUM_VAL_IMAGES)\n",
    "# For now lets just set X_val, y_val to X_test, y_test\n",
    "X_val, y_val = X_test, y_test\n",
    "\n",
    "# Set global variables\n",
    "N_TRAIN = len(X_train)\n",
    "N_VAL = len(X_val)\n",
    "N_TEST = len(X_test)\n",
    "\n",
    "print('Dataset loaded!')\n",
    "DATA_LOADED = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### §2.2: Exploratory Data Analysis (EDA)\n",
    "\n",
    "In this section, we use graphical and numerical techniques to begin uncovering the structure of our data.\n",
    "\n",
    "\n",
    "\n",
    "*   What is the “distribution” of the variable we are trying to predict? \n",
    "*   What does the data look like?\n",
    "\n",
    "\n",
    "#### §2.2.1: Data Visualisation\n",
    "\n",
    "The first thing we do is plot the distribution of the variable we are trying to predict, which is in this case the “class” (image category) e.g dog, cat etc. We are splitting the dataset into 3 sets, as is common for machine learning. The three sets are:\n",
    "\n",
    "\n",
    "\n",
    "*   Training\n",
    "    *   The purpose of this is to train the machine learning model, by inputting data from which the model can learn patterns to predict the target variable (in this case, the image category)\n",
    "*   Validation\n",
    "    *   We use this to “test” our model. Since we will be developing many different types of models, we use this set of images to test between all the different models and decide which one is likely to perform the best on new data\n",
    "*   Testing\n",
    "    *   This is a completely “held out” dataset that the model has never seen before. The point of this dataset is to simulate “live data” to get a better measure of how the model would expect to perform if we were to operationalise it\n",
    "\n",
    "The difference between validation and testing sets is subtle. If we only had two sets: training and testing, then we could keep choosing models that get better on the test set during this experimental, but wont generalise to a different set of data when the model is deployed. This would mean that we pick the best model for a specific set of data, rather than the best model “overall”. The diagram below shows this process.\n",
    "\n",
    "<img src=\"images/machinelearningsets.png\" width=\"600\" title=\"Machine Learning Sets\">\n",
    "\n",
    "> **The button below will plot the distribution of image classes in each of the sets. You should see that there are 5,000 images of each class in the training data. You can use your mouse to hover over each category colour, and isolate a category by clicking the legend on the side**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "init_cell": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd34924441484d009330ab6754bc08e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run code', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_12103732934130089973() {\n",
       "                $('div.cell.code_cell.rendered.selected').next().find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            $('div.cell.code_cell.rendered.selected').find(\"div.input\").hide();\n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_12103732934130089973()\">Toggle show/hide code</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_buttons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a checkpoint\n",
    "checkpoint(0)\n",
    "\n",
    "# Get the counts of each of the image types and put them in a dataframe\n",
    "set_value_counts = pd.DataFrame([\n",
    "    np.unique(y_train, return_counts=True)[1],\n",
    "    np.unique(y_val, return_counts=True)[1],\n",
    "    np.unique(y_test, return_counts=True)[1]\n",
    "], index=['Training Set', 'Validation Set', 'Testing Set'])\n",
    "\n",
    "set_value_counts.columns = set_value_counts.columns.map(CLASS_NAMES)\n",
    "set_value_counts.columns.name = 'Image Class'\n",
    "set_value_counts.index.name = 'Set Name'\n",
    "\n",
    "# Transform the dataframe into a format suitable for altair\n",
    "source = set_value_counts.reset_index().melt('Set Name')\n",
    "\n",
    "# Create a multi selection index with every class pre-initialised\n",
    "selection = alt.selection_multi(\n",
    "    fields=[\"Image Class\"]\n",
    ")\n",
    "color = alt.condition(\n",
    "    selection, alt.Color(\"Image Class:N\", legend=None), alt.value(\"lightgray\")\n",
    ")\n",
    "\n",
    "barchart = (\n",
    "    alt.Chart(source)\n",
    "    .mark_bar()\n",
    "    .encode(x=alt.X(\"Set Name:N\", axis=alt.Axis(labelAngle=-45)),\n",
    "            y=\"value:Q\", color=color, \n",
    "            tooltip=[alt.Tooltip(\"Image Class:N\", title='Image Class'), \n",
    "                     alt.Tooltip(\"value:N\", title='Number of Images')])\n",
    "    .add_selection(selection)\n",
    "    .properties(width=700,height=450)\n",
    ")\n",
    "\n",
    "legend = (\n",
    "    alt.Chart(source)\n",
    "    .mark_point()\n",
    "    .encode(y=alt.Y(\"Image Class:N\", axis=alt.Axis(orient=\"right\")), color=color)\n",
    "    .add_selection(selection)\n",
    ")\n",
    "\n",
    "barchart | legend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we will do is actually plot some of the images for all of the classes and visualise what they look like.\n",
    "\n",
    "> **The button below will plot a sample of the images. You can scroll through the images using the slider**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "init_cell": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd34924441484d009330ab6754bc08e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run code', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_5179278860396219421() {\n",
       "                $('div.cell.code_cell.rendered.selected').next().find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            $('div.cell.code_cell.rendered.selected').find(\"div.input\").hide();\n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_5179278860396219421()\">Toggle show/hide code</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_buttons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, widgets\n",
    "\n",
    "checkpoint(0)\n",
    "\n",
    "SAMPLE_SIZE = 101\n",
    "img_idx_slider = widgets.IntSlider(value=0, min=0, max=SAMPLE_SIZE - 1, description=\"Image index\", \n",
    "                                   layout=widgets.Layout(width='100%', height='50px'))\n",
    "\n",
    "train_images_sample = {class_name: X_train[(y_train == class_idx).squeeze()][:SAMPLE_SIZE] \n",
    "                       for class_idx, class_name in CLASS_NAMES.items()}\n",
    "\n",
    "@interact(i=img_idx_slider)\n",
    "def visualize_prediction(i=0):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(20, 10))\n",
    "    for ax, class_name in zip(axes.flat, CLASS_NAMES.values()):\n",
    "        ax.imshow(train_images_sample[class_name][i].squeeze())\n",
    "        ax.set_title(f\"Class / label: {class_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> Discussion Point</span>\n",
    "\n",
    "**Q1**. Does the data look good based on these plots?\n",
    "\n",
    "\n",
    "\n",
    "*   In the count plot of the images, what do you think a “good” and “bad” distribution of categories would look like?\n",
    "*   Do you think the amount of data is OK?\n",
    "*   What do you think of the image quality?\n",
    "\n",
    "\n",
    "#### §2.2.2: Data Preprocessing\n",
    "\n",
    "One key thing for computers is that we have to transform everything over to numbers - so when we are dealing with image data (or text data) we have to find a way of mapping each of the images into numbers that a computer can understand. Luckily, with RGB images we can do that. RGB Images are made up of pixels, and each pixel is a tuple $(R,G,B) = (x,y,z)$ where $x,y,z$ are all numbers between 0 and 255. That means we can instead represent each image as a _tensor of numbers_. We are going to define operations on them and build machine learning models so that the data can _flow_ through these sets of processes (hence the name _Tensorflow_ by Google, if you've heard of it!). A tensor is basically just an array of numbers.\n",
    "\n",
    "The image below represents this process:\n",
    "\n",
    "<img src=\"images/RGBImage.png\" width=\"600\" title=\"RGB Image converted into a tensor\">\n",
    "\n",
    "It turns out that neural networks learn better when their inputs are standardised, say between 0 and 1. The only preprocessing we do, therefore, is to divide these image arrays by 255 to ensure all the inputs fall between 0 and 1 instead of 0 and 255.\n",
    "\n",
    "Normally, data preprocessing (or _“data cleaning”_ ) is quite a lengthy process. We can manipulate and transform the data in many different ways to extract _“features”_ which we use as an input to our neural networks. This is a combination of domain and problem knowledge, coupled with some standard machine learning and data science knowledge. It is usually an experimental trial-and-error exploratory process.\n",
    "\n",
    "> **The button below will preprocess the dataset, ensuring it is suitable to input into a neural network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "init_cell": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd34924441484d009330ab6754bc08e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run code', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_7988502162873027456() {\n",
       "                $('div.cell.code_cell.rendered.selected').next().find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            $('div.cell.code_cell.rendered.selected').find(\"div.input\").hide();\n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_7988502162873027456()\">Toggle show/hide code</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_buttons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint(0)\n",
    "\n",
    "# Preprocessing - normalise the images\n",
    "X_train, X_val, X_test = X_train / 255.0, X_val / 255.0, X_test / 255.0\n",
    "\n",
    "DATA_PREPROCESSED = True\n",
    "\n",
    "print('Images preprocessed successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## §3: Model Building Phase\n",
    "\n",
    "In this phase, we begin by defining some baseline models to benchmark our performance\n",
    "\n",
    "\n",
    "### §3.1: Baseline Neural Network Model\n",
    "\n",
    "To get a sense of the problem, we start by building a very simple deep neural network model, which takes all the image pixels and flattens them into a row of numbers (instead of a grid). Then, we pass it through several “layers” of the model, combining these numbers together in complex (“non linear”) ways in an attempt to try and match the pixel values with the output class numbers. We hope that by training this network on the data, we can learn how to transform an input image of pixels into the correct output class. The “deepness” in a neural network refers to additional layers in our architecture, but abstractly represents additional layers of complex reasoning. We hope that the early layers in a neural network can identify things like edges, and these are combined later in the network to identify “features” of the image (such as a cat’s whiskers, or a ships chimney). These features would ordinarily be really hard to code for explicitly, so we get a model to implicitly learn them.\n",
    "\n",
    "<img src=\"images/nn_architecture.jpg\" width=\"600\" title=\"Neural Network Architecture\">\n",
    "\n",
    "\n",
    "#### §3.1.1: Model definition\n",
    "\n",
    "We define a basic model somewhat arbitrarily, just to test that the model starts to learn something and we have set everything up appropriately in the code (to _“test our pipeline”_ ). We can always change the model architecture later (and, indeed, we will). \n",
    "\n",
    "\n",
    "#### §3.1.2: Training the model\n",
    "\n",
    "Now we are going to train the machine learning model. This process means we will feed in the training data to the model, it will pass through the model and the model will make a prediction. If the predicted label is wrong, then the model will change its parameters (through a process called_ backpropagation_) in an attempt to correct its mistakes.\n",
    "\n",
    "The way a model knows that it is wrong is due to a **loss function**. You can think of this as a function that captures and encodes the objective we care about. For example, in the loss function there is normally an “error term”, which reflects how far away our prediction is from the true value according to some metric. Thus, our training procedure tries to **minimise the loss** (because we want to **minimise the error** of our model!) The loss function is also called the “cost function” (referencing to economics - we want to minimise our cost - high cost is bad, and low cost is good. Here “cost” is abstract, and nothing to do with money!).\n",
    "\n",
    "You will see below the plots of the model training in real time. You should note that the loss function will decrease, since the model is minimising its error, and the model accuracy will increase, since it is refining its predictive power.\n",
    "\n",
    "The validation data is to check our model performance. If our model is capable enough, it could just learn to memorise all the training examples in the dataset, thus getting zero loss/error and 100% accuracy, but it wouldn’t be able to generalise to new datapoints (in this case, images) outside of the training set! So the validation loss should also decrease, and the accuracy should increase but you would expect there to be a certain point where the model just starts to memorise the training, meaning the error on the validation set goes up again (instead of down) and the accuracy decreases. We use plots like this to decide the best time to stop our model training.\n",
    "\n",
    "<table><tr>\n",
    "    <td> <img src=\"images/loss_curves.png\" alt=\"Typical Loss Curves\" style=\"width: 500px;\"/> </td>\n",
    "    <td> <img src=\"images/accuracy_curves.png\" alt=\"Typical Accuracy Curves\" style=\"width: 400px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "> **The button below will train the baseline neural network model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "init_cell": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd34924441484d009330ab6754bc08e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run code', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_7640983283564751758() {\n",
       "                $('div.cell.code_cell.rendered.selected').next().find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            $('div.cell.code_cell.rendered.selected').find(\"div.input\").hide();\n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_7640983283564751758()\">Toggle show/hide code</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_buttons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint(1)\n",
    "\n",
    "# Create the baseline model described in the text above (TODO)\n",
    "baseline_NN_model = tf.keras.Sequential([\n",
    "    # Flatten RGB image + NN\n",
    "    tf.keras.layers.Flatten(input_shape=IMG_SHAPE, name='Flatten'),\n",
    "    tf.keras.layers.Dense(128, activation='relu', name='Dense_1'),\n",
    "    tf.keras.layers.Dense(64, activation='relu', name='Dense_2'),\n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax', name='Predictions')\n",
    "], name='Baseline_NN_Model')\n",
    "\n",
    "# Print model summary - potentially remove - or print out image here\n",
    "print(baseline_NN_model.summary())\n",
    "\n",
    "# Compile the model - we will use the Adam optimiser and categorical crossentropy loss, logging the accuracy\n",
    "# TODO: Tensorboard here and integrate callbacks into live tracking below? Potentially just a cleaner interface\n",
    "baseline_NN_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")  \n",
    "\n",
    "# Fit the baseline model to the training data, validating against the validation data\n",
    "baseline_NN_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size = 32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[PlotLossesCallback()],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "BASELINE_MODEL_BUILT = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> Discussion Point</span>\n",
    "\n",
    "You should notice that the accuracy increases over time (the plot on the right), and the loss/error decreases over time (the plot on the left) although the plots are quite “jagged”.\n",
    "\n",
    "**Q2**. What do you notice about the accuracy of this model? Is it good enough? What would we expect the performance to be on this task?  \\\n",
    "_Hint: The most basic model we could think of, that is not machine learning at all, is just “randomly guessing the image class” or “always predicting the same class (e.g dog)”. What would the accuracy be in this case? Remember we have 10 classes, and the number of images is the same for each class in the training, validation and testing set?_\n",
    "\n",
    "**Q3**. Do you think accuracy is a good measure of a models performance? \\\n",
    "_Hint: Suppose we only had two classes (“dog” and “cat”) and we had 9500 images of dogs and 500 images of cats. What would the accuracy of a classifier that always predicts “dog” be?_\n",
    "\n",
    "\n",
    "#### §3.1.3: Evaluating the model\n",
    "\n",
    "To test how our model performs we plot a “confusion matrix”. We plot the model performance on the test set of unseen data during testing. The model is introduced to new images, none of which it has ever seen before, and has to make predictions. Luckily, we know the true image classes as labels, so we can compare with what our model predicts vs the actual outcome and plot is as a grid. This is called a _confusion matrix._\n",
    "\n",
    "This array provides a lookup table between the number of images of a certain class X to be predicted as class Y. The diagonal thus represents the number of images that were predicted correctly as class X. The “colour” of each square reflects the value, “hot” colours are high values - corresponding to a large number of images, and “cold” colours are low values - corresponding to a low number of images.\n",
    "\n",
    "> **The button below will plot the confusion matrix for the Neural Network model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "init_cell": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd34924441484d009330ab6754bc08e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run code', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_5266136930530837152() {\n",
       "                $('div.cell.code_cell.rendered.selected').next().find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            $('div.cell.code_cell.rendered.selected').find(\"div.input\").hide();\n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_5266136930530837152()\">Toggle show/hide code</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_buttons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint(2)\n",
    "\n",
    "baseline_NN_model_probs, baseline_NN_model_preds = get_model_outputs(baseline_NN_model, X_test)\n",
    "print_confusion_matrix(baseline_NN_model_preds, y_test, class_names=CLASS_NAMES.values())\n",
    "\n",
    "NN_CONFUSION_MATRIX = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> Discussion Point</span>\n",
    "\n",
    "**Q4**. What would a good confusion matrix / heatmap look like in this context?\n",
    "\n",
    "_Hint: What do the numbers on the diagonal represent?_\n",
    "\n",
    "\n",
    "\n",
    "*   What about if the context were different, and instead the labels were “risk scores” from 1 to 5. What would we want our confusion matrix to look like in that case?\n",
    "\n",
    "**Q5. **Which classes are “most confused” by the model? In other words, which classes does the model predict as wrong often? Do these seem sensible? \n",
    "\n",
    "_Hint: Look for the high numbers that aren’t on the diagonal._\n",
    "\n",
    "#### §3.1.4: Exploring the models predictions\n",
    "\n",
    "We can also plot the models predictions on unseen images (i.e images that the model was not trained on). The bar chart on the left shows the probabilities of each of the classes, with the golden bar being the predicted class.\n",
    "\n",
    "> **The button below will plot the image predictions for the Neural Network model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "init_cell": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd34924441484d009330ab6754bc08e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run code', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_6644402573619809427() {\n",
       "                $('div.cell.code_cell.rendered.selected').next().find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            $('div.cell.code_cell.rendered.selected').find(\"div.input\").hide();\n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_6644402573619809427()\">Toggle show/hide code</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_buttons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint(3)\n",
    "## Nice image slider: https://sanjayasubedi.com.np/deeplearning/tensorflow-2-first-neural-network-for-fashion-mnist/\n",
    "img_idx_slider = widgets.IntSlider(value=0, min=0, max=len(X_test)-1, description=\"Image index\",\n",
    "                                   layout=widgets.Layout(width='100%', height='50px'))\n",
    "\n",
    "@interact(i=img_idx_slider)\n",
    "def visualize_prediction(i=0):\n",
    "    img_probs, img_pred = baseline_NN_model_probs[i], baseline_NN_model_preds[i].squeeze()\n",
    "    fix, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax1.imshow(X_test[i])\n",
    "    ax1.set_title(f\"Label: {CLASS_NAMES[y_test[i][0]]}\")\n",
    "    ax1.set_xlabel(f\"Prediction: {CLASS_NAMES[img_pred]}\")\n",
    "\n",
    "    sns.barplot(x=list(CLASS_NAMES.values()), y=img_probs*100, ax=ax2, \n",
    "                palette=['grey' if (prob < max(img_probs)) else 'gold' for prob in img_probs])\n",
    "    plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> Discussion Point</span>\n",
    "\n",
    "**Q6**. Are the predictions of the model good? Why / why not?\n",
    "\n",
    "\n",
    "## §4: Model Iteration Phase\n",
    "\n",
    "Now we have been through the process of building one model, we can quickly iterate on our ideas to try and build a better one! This process is mainly trial and error and “following your nose” as a data scientist, as well as reading through the literature at similar architectures that worked well.\n",
    "\n",
    "\n",
    "### §4.1: Building a model from the literature\n",
    "\n",
    "Upon reading the literature, we find that a model called VGG-16 works well. VGG-16 is a type of Convolutional Neural Network (CNN), which is a particular structure proven to be good at image recognition tasks. It takes in an image, like the neural network before, but instead of just learning how to map the whole image onto the output image classes, it learns smaller squares/grids (called filters) from the image, which it applies to the input image in order to understand what is in the image. For example, it might learn to learn a “ear-like” grid as an intermediate feature, and the model will check if any incoming image has an ear like feature, meaning it is more likely to be an animal than, say, a car.\n",
    "\n",
    "Don’t worry if you don’t understand the specifics, the key takeaway is that this model provides a bit more structure to the learning process, meaning we can learn a better mapping from images to classes than from the images alone (as in the previous model) from the same amount of data. If we had much more data then, theoretically, the model we used before would achieve the same performance as this one, but we help out the learning process by defining additional structure to enable the model to learn more effectively from the same amount of data. An example CNN architecture is shown below:\n",
    "\n",
    "<img src=\"images/cnn_architecture.jpeg\" width=\"600\" title=\"CNN Architecture\">\n",
    "\n",
    "\n",
    "### §4.2: Training the model\n",
    "\n",
    "Since this model is more complicated than the previous model, it would _take too long to train for this workshop_. Instead, we have already trained the model for you. Instead, we will show you what training would have looked like (sped up) and then we will load the model from our _pre-trained version_.\n",
    "\n",
    "In machine learning, we can train a model and save it, then share it between different people so that they don’t have to go through the entire training process again. There are also some more advanced techniques where we can reuse “parts of a model” and training other bits of additional architecture in a modular way, and this is referred to as _transfer learning _since we are transferring some parts of a model over (and, in a sense, transferring the “knowledge of an already trained model” and reusing this on our task).\n",
    "\n",
    "> **The button below will simulate the training of the more advanced “Convolutional Neural Network” (CNN)  VGG-16 architecture to show you what this training process would have looked like. This is not training in real time and it is sped up. At the end, we load the model that we have trained previously so you can use it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "init_cell": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd34924441484d009330ab6754bc08e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run code', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_5306985550521834112() {\n",
       "                $('div.cell.code_cell.rendered.selected').next().find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            $('div.cell.code_cell.rendered.selected').find(\"div.input\").hide();\n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_5306985550521834112()\">Toggle show/hide code</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_buttons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the VGG-16 like model\n",
    "vgg_model = tf.keras.Sequential([ \n",
    "    # Define the model input explicitly here\n",
    "    tf.keras.layers.Input(IMG_SHAPE),\n",
    "    \n",
    "    # VGG-16 normally expects 224x224 images, if we wanted we could upsample the images, but they are already blurry!\n",
    "    #tf.keras.layers.Lambda(lambda x: tf.image.resize(x, (224,224)), input_shape = IMG_SHAPE),\n",
    "    \n",
    "    # Block 1\n",
    "    tf.keras.layers.Convolution2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\", \n",
    "                                  name='block1_conv1'),\n",
    "    tf.keras.layers.Convolution2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\", \n",
    "                                  name='block1_conv2'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='block1_pool'),\n",
    "\n",
    "    # Block 2\n",
    "    tf.keras.layers.Convolution2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\", \n",
    "                                  name='block2_conv1'),\n",
    "    tf.keras.layers.Convolution2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\", \n",
    "                                  name='block2_conv2'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='block2_pool'),\n",
    "\n",
    "    # Block 3\n",
    "    tf.keras.layers.Convolution2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", \n",
    "                                  name='block3_conv1'),\n",
    "    tf.keras.layers.Convolution2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", \n",
    "                                  name='block3_conv2'),\n",
    "    tf.keras.layers.Convolution2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", \n",
    "                                  name='block3_conv3'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='block3_pool'),\n",
    " \n",
    "    # Normally, VGG-16 has 5 Conv blocks, but we shorten this to 3 given the input image size and to ensure\n",
    "    # inference is somewhat quicker. It also turns out this gets quite high accuracy whilst still being lightweight\n",
    "    # - this is a process of trial and error!\n",
    "#     # Block 4\n",
    "#     tf.keras.layers.Convolution2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", \n",
    "#                                   name='block4_conv1'),\n",
    "#     tf.keras.layers.Convolution2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", \n",
    "#                                   name='block4_conv2'),\n",
    "#     tf.keras.layers.Convolution2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", \n",
    "#                                   name='block4_conv3'),\n",
    "#     tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='block3_pool'),\n",
    "    \n",
    "#     # Block 5\n",
    "#     tf.keras.layers.Convolution2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", \n",
    "#                                   name='block5_conv1'),\n",
    "#     tf.keras.layers.Convolution2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", \n",
    "#                                   name='block5_conv2'),\n",
    "#     tf.keras.layers.Convolution2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", \n",
    "#                                   name='block5_conv3'),\n",
    "#     tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='block4_pool'),\n",
    "\n",
    "    # Add classification layers on top of it, projecting the pretrained base model knowledge directly onto the classes\n",
    "    # This is our initial attempt (one dense layer for a linear projection) but this is up to experimentation\n",
    "    # The number of hidden units is a hyperparameter here!\n",
    "    tf.keras.layers.Flatten(name='flatten'),\n",
    "    tf.keras.layers.Dense(256, activation='relu', name='dense_projection'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax', name='predictions')\n",
    "], name='VGG-16 Adapted Model')\n",
    "\n",
    "# Imitate the training of the model by reading the output from a previous training session\n",
    "liveplot = PlotLosses(skip_first=0)\n",
    "with open('VGG_results.txt', 'r') as vgg_results:\n",
    "    for line_count, line in enumerate(vgg_results):\n",
    "        # If we have an even line then it will say Epoch x/50 - extract the epoch number\n",
    "        if line_count % 2 == 0:\n",
    "            epoch_num = line_count // 2 + 1\n",
    "        else:\n",
    "            loss, acc, val_loss, val_acc = (float(line.split(\": \")[metric_num][:6]) for metric_num in range(1,5))\n",
    "            liveplot.update({\n",
    "                'loss': loss,\n",
    "                'val_loss': val_loss,\n",
    "                'accuracy': acc,\n",
    "                'val_accuracy': val_acc\n",
    "            })\n",
    "            liveplot.draw()\n",
    "            time.sleep(1.)     \n",
    "\n",
    "# Load the pretrained weights\n",
    "vgg_model.load_weights('cifar10-vgg16-pretrained.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### §4.3: Evaluating and comparing the models\n",
    "\n",
    "As before, we will evaluate the model, using the same techniques as before.\n",
    "\n",
    "\n",
    "#### §4.3.1: Plotting the confusion matrix\n",
    "\n",
    "As for the previous model, we plot a confusion matrix.\n",
    "\n",
    "> **The button below will plot the confusion matrix for the VGG-16 CNN model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "init_cell": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd34924441484d009330ab6754bc08e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run code', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_11514243909102851161() {\n",
       "                $('div.cell.code_cell.rendered.selected').next().find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            $('div.cell.code_cell.rendered.selected').find(\"div.input\").hide();\n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_11514243909102851161()\">Toggle show/hide code</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_buttons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model_probs, vgg_model_preds = get_model_outputs(vgg_model, X_test)\n",
    "print_confusion_matrix(vgg_model_preds, y_test, class_names=CLASS_NAMES.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> Discussion Point</span>\n",
    "\n",
    "**Q7**. Is this confusion matrix better or worse than before? Why?\n",
    "\n",
    "**Q8**. As before, Which classes are “most confused” by the model? In other words, which classes does the model predict as wrong often? Do these seem sensible? \n",
    "\n",
    "_Hint: Look for the high numbers that aren’t on the diagonal._\n",
    "\n",
    "\n",
    "#### §4.3.2: Comparing model predictions\n",
    "\n",
    "We now compare the previous models on new data. We run the new images (the “test data”) through each of the models and get them to make predictions, and plot them in a bar chart. The golden bar indicates the chosen (“winning”) class and each bar represents the probability of being that class according to the model. We also plot the image. You can scroll through the images to see how the models predict using the slider.\n",
    "\n",
    "> **The button below will plot the models predictions on new data as a bar chart, representing the probability of being each of the classes, with the golden bar representing the “winning” (predicted) class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "init_cell": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd34924441484d009330ab6754bc08e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run code', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_562012022477668264() {\n",
       "                $('div.cell.code_cell.rendered.selected').next().find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            $('div.cell.code_cell.rendered.selected').find(\"div.input\").hide();\n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_562012022477668264()\">Toggle show/hide code</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_buttons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_idx_slider = widgets.IntSlider(value=0, min=0, max=len(X_test)-1, description=\"Image index\",\n",
    "                                   layout=widgets.Layout(width='100%', height='50px'))\n",
    "\n",
    "@interact(i=img_idx_slider)\n",
    "def visualize_prediction(i=0):\n",
    "    baseline_img_probs, baseline_img_pred = baseline_NN_model_probs[i], baseline_NN_model_preds[i].squeeze()\n",
    "    vgg_img_probs, vgg_img_pred = vgg_model_probs[i], vgg_model_preds[i].squeeze()\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    ax1.imshow(X_test[i])\n",
    "    ax1.set_title(f\"Label: {CLASS_NAMES[y_test[i][0]]}\")\n",
    "    ax1.set_xlabel(f\"Baseline Prediction: {CLASS_NAMES[baseline_img_pred]} \\n VGG Prediction: {CLASS_NAMES[vgg_img_pred]}\")\n",
    "\n",
    "    sns.barplot(x=list(CLASS_NAMES.values()), y=baseline_img_probs*100, ax=ax2, \n",
    "                palette=['grey' if (prob < max(baseline_img_probs)) else 'gold' for prob in baseline_img_probs])\n",
    "    ax2.set_title(\"Baseline Neural Network Model Prediction\")\n",
    "    \n",
    "    sns.barplot(x=list(CLASS_NAMES.values()), y=vgg_img_probs*100, ax=ax3, \n",
    "                palette=['grey' if (prob < max(vgg_img_probs)) else 'gold' for prob in vgg_img_probs])\n",
    "    ax3.set_title(\"VGG-16 Model Prediction\")\n",
    "    \n",
    "    for axis in [ax2, ax3]:\n",
    "        axis.set_ylim([0,100])\n",
    "        axis.set_xticklabels(axis.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> Discussion Point</span>\n",
    "\n",
    "**Q9**. Which model is better?\n",
    "\n",
    "_Extra discussion point: The VGG-16 model (rightmost plot) is more “confident” in its predictions - is this necessarily a good thing?_\n",
    "\n",
    "\n",
    "## §4: Extensions\n",
    "\n",
    "\n",
    "### §4.1: Analysing “why” a model makes its prediction\n",
    "\n",
    "A popular area of research in machine learning is “Explainable AI”, a branch of algorithm research that interrogates “why” a model makes its prediction. It has often been said that some AI systems are “black boxes”, in that they make predictions but the reasons and motivations on which these predictions are based are hard to decipher and comprehend. \n",
    "\n",
    "There are many different ways to try and explain what a model has learned, and no algorithm will ever be perfect, but we can use it as a helpful indicator to diagnose some of the decision making used by these systems.\n",
    "\n",
    "We can plot a heatmap, indicating the regions that a model paid “attention to” when making its decisions. The regions of a “hot colour” indicate regions that correspond to a high score for the predicted class - i.e it was the areas for which the model strongly based its decision on.\n",
    "\n",
    "> **The button below will plot the models predictions and associated heatmaps on new images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "init_cell": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd34924441484d009330ab6754bc08e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run code', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_16347925999078069572() {\n",
       "                $('div.cell.code_cell.rendered.selected').next().find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            $('div.cell.code_cell.rendered.selected').find(\"div.input\").hide();\n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_16347925999078069572()\">Toggle show/hide code</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_buttons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_idx_slider = widgets.IntSlider(value=0, min=0, max=SAMPLE_SIZE - 1, description=\"Image index\", \n",
    "                                   layout=widgets.Layout(width='100%', height='50px'))\n",
    "\n",
    "test_images_sample = {class_name: X_test[(y_test == class_idx).squeeze()][:SAMPLE_SIZE] \n",
    "                       for class_idx, class_name in CLASS_NAMES.items()}\n",
    "test_images_pred_sample = {class_name: vgg_model.predict(class_sample) \n",
    "                           for class_name, class_sample in test_images_sample.items()}\n",
    "explainer = GradCAM()\n",
    "\n",
    "@interact(i=img_idx_slider)\n",
    "def visualize_prediction(i=0):\n",
    "    fig = plt.figure(figsize=(20, 8))\n",
    "    outer = gridspec.GridSpec(2, 5, wspace=0.2, hspace=0.2)\n",
    "    for class_idx, class_name in CLASS_NAMES.items():\n",
    "        inner = gridspec.GridSpecFromSubplotSpec(2, 1,\n",
    "                                                 subplot_spec=outer[class_idx], \n",
    "                                                 wspace=0.1, hspace=0.1)\n",
    "        test_image = test_images_sample[class_name][i].squeeze()\n",
    "        data = ([test_image], None)\n",
    "        # Compute GradCAM on VGG16\n",
    "        grid = explainer.explain(\n",
    "            data, vgg_model, class_index=0, layer_name=\"block3_conv3\"\n",
    "        )\n",
    "        for subplot_num, image in enumerate([test_image, grid]):\n",
    "            ax = plt.Subplot(fig, inner[subplot_num])\n",
    "            ax.imshow(image)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            fig.add_subplot(ax)\n",
    "            if subplot_num == 0:\n",
    "                predicted_class = CLASS_NAMES[test_images_pred_sample[class_name][i].argmax()]\n",
    "                ax.set_title(f'Actual Class: {class_name} \\n Predicted Class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> Discussion Point</span>\n",
    "\n",
    "**Q10**. What do you notice in the plots? \n",
    "\n",
    "\n",
    "## References\n",
    "\n",
    "[1] [http://karpathy.github.io/2011/04/27/manually-classifying-cifar10/](http://karpathy.github.io/2011/04/27/manually-classifying-cifar10/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "### The CIFAR-100 dataset\n",
    "This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a \"fine\" label (the class to which it belongs) and a \"coarse\" label (the superclass to which it belongs).\n",
    "Here is the list of classes in the CIFAR-100:\n",
    "\n",
    "\n",
    "| Superclass | Classes |\n",
    "|:----------|:--------:|\n",
    "| aquatic mammals | beaver, dolphin, otter, seal, whale |\n",
    "| fish | aquarium fish, flatfish, ray, shark, trout |\n",
    "| flowers | orchids, poppies, roses, sunflowers, tulips |\n",
    "| food containers | bottles, bowls, cans, cups, plates |\n",
    "| fruit and vegetables | apples, mushrooms, oranges, pears, sweet peppers |\n",
    "| household electrical devices | clock, computer keyboard, lamp, telephone, television |\n",
    "| household furniture | bed, chair, couch, table, wardrobe |\n",
    "| insects | bee, beetle, butterfly, caterpillar, cockroach |\n",
    "| large carnivores | bear, leopard, lion, tiger, wolf |\n",
    "| large man-made outdoor things | bridge, castle, house, road, skyscraper |\n",
    "| large natural outdoor scenes | cloud, forest, mountain, plain, sea |\n",
    "| large omnivores and herbivores | camel, cattle, chimpanzee, elephant, kangaroo |\n",
    "| medium-sized mammals | fox, porcupine, possum, raccoon, skunk |\n",
    "| non-insect invertebrates | crab, lobster, snail, spider, worm |\n",
    "| people | baby, boy, girl, man, woman |\n",
    "| reptiles | crocodile, dinosaur, lizard, snake, turtle |\n",
    "| small mammals | hamster, mouse, rabbit, shrew, squirrel |\n",
    "| trees | maple, oak, palm, pine, willow |\n",
    "| vehicles 1 | bicycle, bus, motorcycle, pickup truck, train |\n",
    "| vehicles 2 | lawn-mower, rocket, streetcar, tank, tractor |\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
